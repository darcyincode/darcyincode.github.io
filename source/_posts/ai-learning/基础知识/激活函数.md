---
title: 激活函数（Activation Functions）
date: 2026-02-04
order: 2
categories:
  - [大模型, 数学基础]
tags:
  - 深度学习
  - 神经网络
  - AI
---

# 激活函数（Activation Functions）

## 什么是激活函数？

激活函数是神经网络中的**非线性变换函数**，应用于神经元的加权和之后，决定神经元是否被"激活"以及激活的程度。

**计算流程：**
```
输入 → 加权求和 → 激活函数 → 输出
 x   →  z = wx+b  →  a = f(z)  →  a
```

**数学表达：**
$$z = \mathbf{w}^T \mathbf{x} + b$$
$$a = f(z)$$

其中 $f$ 就是激活函数。

---

## 为什么需要激活函数？

### 1. 引入非线性

**没有激活函数的问题：**

假设有一个两层网络，每层都是线性变换：
$$h = W_1 x + b_1$$
$$y = W_2 h + b_2$$

代入得：
$$y = W_2(W_1 x + b_1) + b_2 = W_2 W_1 x + W_2 b_1 + b_2$$

令 $W = W_2 W_1$，$b = W_2 b_1 + b_2$，则：
$$y = Wx + b$$

**结论：** 多层线性变换仍然是线性的！深度神经网络退化为单层线性模型，无法拟合复杂的非线性模式。

### 2. 解决实际问题的能力

- **线性模型只能解决线性可分问题**（如简单的直线分类）
- **非线性激活函数使网络能学习复杂模式**（如XOR问题、图像识别）
- **非线性堆叠产生万能近似能力**（理论上可逼近任意连续函数）

---

## 常用激活函数

### 1. Sigmoid（逻辑函数）

#### 公式
$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

#### 导数
$$\sigma'(z) = \sigma(z)(1 - \sigma(z))$$

#### 特性
- **输出范围**：(0, 1)
- **零中心化**：否（输出恒为正）
- **形状**：S型平滑曲线

#### 函数图

<div class="chart-data" style="display:none;">
{
  "type": "line",
  "data": {
    "labels": [-6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6],
    "datasets": [{
      "label": "σ(z) = 1/(1+e^(-z))",
      "data": [0.002, 0.007, 0.018, 0.047, 0.119, 0.269, 0.5, 0.731, 0.881, 0.953, 0.982, 0.993, 0.998],
      "borderColor": "#6D3896",
      "backgroundColor": "rgba(109, 56, 150, 0.1)",
      "borderWidth": 3,
      "tension": 0.4,
      "pointRadius": 0,
      "fill": true
    }]
  },
  "options": {
    "responsive": true,
    "plugins": {
      "title": {
        "display": true,
        "text": "Sigmoid激活函数",
        "font": { "size": 16 }
      },
      "legend": { "display": false }
    },
    "scales": {
      "x": { "title": { "display": true, "text": "z" } },
      "y": { 
        "min": 0, 
        "max": 1,
        "title": { "display": true, "text": "σ(z)" }
      }
    }
  }
}
</div>

#### 优点
✅ 输出在(0,1)，适合二分类输出层  
✅ 平滑可微  
✅ 有明确的概率解释

#### 缺点
❌ **梯度消失**：两端梯度接近0，深层网络难以训练  
❌ **非零中心化**：输出恒为正，导致梯度更新效率低  
❌ **计算开销大**：包含指数运算

#### 适用场景
- ✅ 二分类问题的输出层
- ❌ 隐藏层（已基本被ReLU取代）

---

### 2. Tanh（双曲正切）

#### 公式
$$\tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}} = 2\sigma(2z) - 1$$

#### 导数
$$\tanh'(z) = 1 - \tanh^2(z)$$

#### 特性
- **输出范围**：(-1, 1)
- **零中心化**：是
- **形状**：S型，比Sigmoid陡峭

#### 函数图

<div class="chart-data" style="display:none;">
{
  "type": "line",
  "data": {
    "labels": [-4, -3, -2, -1, 0, 1, 2, 3, 4],
    "datasets": [{
      "label": "tanh(z)",
      "data": [-0.999, -0.995, -0.964, -0.762, 0, 0.762, 0.964, 0.995, 0.999],
      "borderColor": "#6D3896",
      "backgroundColor": "rgba(109, 56, 150, 0.1)",
      "borderWidth": 3,
      "tension": 0.4,
      "pointRadius": 0,
      "fill": true
    }]
  },
  "options": {
    "responsive": true,
    "plugins": {
      "title": {
        "display": true,
        "text": "Tanh激活函数",
        "font": { "size": 16 }
      },
      "legend": { "display": false }
    },
    "scales": {
      "x": { "title": { "display": true, "text": "z" } },
      "y": { 
        "min": -1.2, 
        "max": 1.2,
        "title": { "display": true, "text": "tanh(z)" }
      }
    }
  }
}
</div>

#### 优点
✅ **零中心化**：比Sigmoid收敛更快  
✅ 输出范围对称  
✅ 梯度比Sigmoid大

#### 缺点
❌ **仍有梯度消失问题**：两端梯度接近0  
❌ 计算开销大（包含指数运算）

#### 适用场景
- ✅ RNN/LSTM的隐藏层
- ⚠️ 前馈网络隐藏层（ReLU更常用）

---

### 3. ReLU（修正线性单元）

#### 公式
$$\text{ReLU}(z) = \max(0, z) = \begin{cases} z & \text{if } z > 0 \\ 0 & \text{if } z \leq 0 \end{cases}$$

#### 导数
$$\text{ReLU}'(z) = \begin{cases} 1 & \text{if } z > 0 \\ 0 & \text{if } z \leq 0 \end{cases}$$

#### 特性
- **输出范围**：[0, +∞)
- **线性区域**：正半轴为线性
- **稀疏激活**：约50%的神经元被激活

#### 函数图

<div class="chart-data" style="display:none;">
{
  "type": "line",
  "data": {
    "labels": [-3, -2, -1, 0, 1, 2, 3, 4, 5],
    "datasets": [{
      "label": "ReLU(z) = max(0, z)",
      "data": [0, 0, 0, 0, 1, 2, 3, 4, 5],
      "borderColor": "#B4E600",
      "backgroundColor": "rgba(180, 230, 0, 0.1)",
      "borderWidth": 3,
      "tension": 0,
      "pointRadius": 0,
      "fill": true
    }]
  },
  "options": {
    "responsive": true,
    "plugins": {
      "title": {
        "display": true,
        "text": "ReLU激活函数",
        "font": { "size": 16 }
      },
      "legend": { "display": false }
    },
    "scales": {
      "x": { "title": { "display": true, "text": "z" } },
      "y": { 
        "min": -0.5, 
        "max": 5.5,
        "title": { "display": true, "text": "ReLU(z)" }
      }
    }
  }
}
</div>

#### 优点
✅ **计算简单**：只需比较和取最大值  
✅ **缓解梯度消失**：正区域梯度恒为1  
✅ **稀疏激活**：部分神经元输出0，减少计算  
✅ **收敛速度快**：比Sigmoid/Tanh快约6倍

#### 缺点
❌ **神经元死亡（Dying ReLU）**：负区域梯度为0，一旦进入负区域可能永远无法恢复  
❌ **非零中心化**：输出恒为正  
❌ **无界输出**：可能导致激活值爆炸

#### 适用场景
- ✅ **深度前馈网络的隐藏层**（最常用）
- ✅ CNN的卷积层
- ⚠️ 需要配合合理的学习率和初始化

---

### 4. Leaky ReLU

#### 公式
$$\text{Leaky ReLU}(z) = \max(\alpha z, z) = \begin{cases} z & \text{if } z > 0 \\ \alpha z & \text{if } z \leq 0 \end{cases}$$

通常 $\alpha = 0.01$

#### 导数
$$\text{Leaky ReLU}'(z) = \begin{cases} 1 & \text{if } z > 0 \\ \alpha & \text{if } z \leq 0 \end{cases}$$

#### 函数图

<div class="chart-data" style="display:none;">
{
  "type": "line",
  "data": {
    "labels": [-3, -2, -1, 0, 1, 2, 3],
    "datasets": [
      {
        "label": "Leaky ReLU (α=0.1)",
        "data": [-0.3, -0.2, -0.1, 0, 1, 2, 3],
        "borderColor": "#6D3896",
        "borderWidth": 3,
        "tension": 0,
        "pointRadius": 0
      },
      {
        "label": "ReLU",
        "data": [0, 0, 0, 0, 1, 2, 3],
        "borderColor": "#ccc",
        "borderWidth": 2,
        "borderDash": [5, 5],
        "tension": 0,
        "pointRadius": 0
      }
    ]
  },
  "options": {
    "responsive": true,
    "plugins": {
      "title": {
        "display": true,
        "text": "Leaky ReLU vs ReLU",
        "font": { "size": 16 }
      }
    },
    "scales": {
      "x": { "title": { "display": true, "text": "z" } },
      "y": { "title": { "display": true, "text": "f(z)" } }
    }
  }
}
</div>

#### 优点
✅ **缓解Dying ReLU问题**：负区域仍有梯度  
✅ 保留ReLU的计算优势  

#### 缺点
❌ 超参数α需要调整  

#### 适用场景
- ✅ 替代ReLU，特别是出现神经元死亡时

---

### 5. GELU（高斯误差线性单元）

#### 公式
$$\text{GELU}(z) = z \cdot \Phi(z)$$

其中 $\Phi(z)$ 是标准正态分布的累积分布函数。

**近似公式**（更常用）：
$$\text{GELU}(z) \approx 0.5z\left(1 + \tanh\left[\sqrt{\frac{2}{\pi}}\left(z + 0.044715z^3\right)\right]\right)$$

#### 函数图

<div class="chart-data" style="display:none;">
{
  "type": "line",
  "data": {
    "labels": [-3, -2.5, -2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2, 2.5, 3],
    "datasets": [
      {
        "label": "GELU",
        "data": [-0.004, -0.024, -0.077, -0.184, -0.159, -0.154, 0, 0.346, 0.841, 1.316, 1.923, 2.476, 2.996],
        "borderColor": "#6D3896",
        "borderWidth": 3,
        "tension": 0.4,
        "pointRadius": 0
      },
      {
        "label": "ReLU",
        "data": [0, 0, 0, 0, 0, 0, 0, 0.5, 1, 1.5, 2, 2.5, 3],
        "borderColor": "#ccc",
        "borderWidth": 2,
        "borderDash": [5, 5],
        "tension": 0,
        "pointRadius": 0
      }
    ]
  },
  "options": {
    "responsive": true,
    "plugins": {
      "title": {
        "display": true,
        "text": "GELU vs ReLU",
        "font": { "size": 16 }
      }
    },
    "scales": {
      "x": { "title": { "display": true, "text": "z" } },
      "y": { "title": { "display": true, "text": "f(z)" } }
    }
  }
}
</div>

#### 优点
✅ **平滑非线性**：在0附近更平滑  
✅ **避免神经元死亡**：负区域仍有小激活  
✅ **Transformer标配**：BERT、GPT等模型默认激活函数

#### 缺点
❌ **计算复杂**：涉及误差函数或Tanh近似

#### 适用场景
- ✅ **Transformer模型**（BERT、GPT）
- ✅ **NLP任务**
- ✅ 需要平滑激活的深度网络

---

### 6. Swish（自门控激活函数）

#### 公式
$$\text{Swish}(z) = z \cdot \sigma(z) = \frac{z}{1 + e^{-z}}$$

#### 函数图

<div class="chart-data" style="display:none;">
{
  "type": "line",
  "data": {
    "labels": [-3, -2.5, -2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2, 2.5, 3],
    "datasets": [
      {
        "label": "Swish",
        "data": [-0.143, -0.184, -0.238, -0.315, -0.269, -0.189, 0, 0.311, 0.731, 1.316, 1.881, 2.474, 2.958],
        "borderColor": "#B4E600",
        "borderWidth": 3,
        "tension": 0.4,
        "pointRadius": 0
      },
      {
        "label": "ReLU",
        "data": [0, 0, 0, 0, 0, 0, 0, 0.5, 1, 1.5, 2, 2.5, 3],
        "borderColor": "#ccc",
        "borderWidth": 2,
        "borderDash": [5, 5],
        "tension": 0,
        "pointRadius": 0
      }
    ]
  },
  "options": {
    "responsive": true,
    "plugins": {
      "title": {
        "display": true,
        "text": "Swish vs ReLU",
        "font": { "size": 16 }
      }
    },
    "scales": {
      "x": { "title": { "display": true, "text": "z" } },
      "y": { "title": { "display": true, "text": "f(z)" } }
    }
  }
}
</div>

#### 优点
✅ **平滑无界**：比ReLU更平滑  
✅ **自门控机制**：隐式学习门控  
✅ **性能优异**：在图像分类等任务上超越ReLU

#### 缺点
❌ **计算稍复杂**：包含Sigmoid

#### 适用场景
- ✅ **深度CNN**（如EfficientNet）
- ✅ 图像分类任务

---

## 激活函数对比总结

| 激活函数 | 输出范围 | 零中心 | 计算复杂度 | 梯度消失 | 主要应用 |
|----------|----------|--------|------------|----------|----------|
| **Sigmoid** | (0, 1) | ❌ | 高 | ✅ 严重 | 二分类输出层 |
| **Tanh** | (-1, 1) | ✅ | 高 | ✅ 存在 | RNN/LSTM隐藏层 |
| **ReLU** | [0, +∞) | ❌ | 低 | ❌ 缓解 | 深度网络隐藏层（最常用） |
| **Leaky ReLU** | (-∞, +∞) | ❌ | 低 | ❌ 缓解 | 替代ReLU |
| **GELU** | 约(-0.17z, +∞) | 接近 | 高 | ❌ 缓解 | Transformer/NLP |
| **Swish** | 约(-0.28z, +∞) | ❌ | 中 | ❌ 缓解 | 深度CNN |

---

## 如何选择激活函数？

### 1. 隐藏层
- **首选 ReLU**：简单高效，适合大多数场景
- **ReLU失效时**：尝试 Leaky ReLU
- **追求极致性能**：GELU（Transformer）、Swish（CNN）
- **RNN/LSTM**：Tanh（配合Sigmoid门控）

### 2. 输出层
- **二分类**：Sigmoid
- **多分类**：Softmax
- **回归任务**：线性（或ReLU保证非负）

### 3. 调优建议
1. **从ReLU开始**：快速验证模型架构
2. **监控神经元死亡**：大量0输出时切换到Leaky ReLU
3. **跟随SOTA模型**：
   - Transformer → GELU
   - ResNet/EfficientNet → ReLU/Swish
   - LSTM → Tanh
4. **注意初始化**：配合He初始化（ReLU系）或Xavier初始化（Tanh/Sigmoid）

---

**文档版本**：v2.0（Chart.js图表版）  
**最后更新**：2026年2月
